{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7cbc272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing \n",
    "from collections import Counter\n",
    "\n",
    "data=pd.read_csv(\"Creditcard_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0e935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data\n",
    "Y=data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05ba6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "011cb254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 763), (1, 763)] (1526,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE()\n",
    "X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
    "print(sorted(Counter(Y_resampled).items()),Y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6eb2d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def yamane_sample_size(population_size, margin_of_error):\n",
    "    n = population_size / (1 + population_size * math.pow(margin_of_error, 2))\n",
    "    return int(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bed9cb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_size = 1526\n",
    "margin_of_error = 0.05\n",
    "\n",
    "sample_size = yamane_sample_size(population_size, margin_of_error)\n",
    "\n",
    "\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "509100ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 316\n",
    "\n",
    "# Set the sampling interval \"k\" as the square root of the number of rows in the dataset\n",
    "k = int(math.sqrt(n))\n",
    "\n",
    "# Select every \"k\" row starting from a random index in the dataset\n",
    "X_syssample = X_resampled.iloc[::k]\n",
    "Y_syssample= Y_resampled.iloc[::k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7eb28cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_syssample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "007fec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_syssample, Y_syssample, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4b81d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr-> 0.8333333333333334\n",
      "dt-> 0.8333333333333334\n",
      "knn-> 0.7222222222222222\n",
      "nb-> 0.7777777777777778\n",
      "rf-> 0.9444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhpuri/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "X_test_prediction = lr.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"lr->\",test_data_accuracy)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "X_test_prediction = dt.predict(X_test)\n",
    "test_data_accuracyd = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"dt->\",test_data_accuracy)\n",
    "\n",
    "knn =  KNeighborsClassifier( n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "X_test_prediction = knn.predict(X_test)\n",
    "test_data_accuracyk = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"knn->\",test_data_accuracyk)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "X_test_prediction = nb.predict(X_test)\n",
    "test_data_accuracynb = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"nb->\",test_data_accuracynb)\n",
    "\n",
    "randfor = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "randfor.fit(X_train, y_train)\n",
    "X_test_prediction = randfor.predict(X_test)\n",
    "test_data_accuracyr = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"rf->\",test_data_accuracyr)\n",
    "\n",
    "sampling1 = [test_data_accuracy*100,test_data_accuracyd*100,\n",
    "                                         test_data_accuracyk*100,test_data_accuracynb*100,\n",
    "                                         test_data_accuracyr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e93a7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "sample_size = 316\n",
    "\n",
    "X_sample= X_resampled.sample(n=sample_size)\n",
    "Y_sample= Y_resampled.sample(n=sample_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f54b020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, Y_sample, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73e0fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr-> 0.578125\n",
      "dt-> 0.578125\n",
      "knn-> 0.4375\n",
      "nb-> 0.4375\n",
      "rf-> 0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhpuri/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "X_test_prediction = lr.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"lr->\",test_data_accuracy)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "X_test_prediction = dt.predict(X_test)\n",
    "test_data_accuracyd = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"dt->\",test_data_accuracy)\n",
    "\n",
    "knn =  KNeighborsClassifier( n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "X_test_prediction = knn.predict(X_test)\n",
    "test_data_accuracyk = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"knn->\",test_data_accuracyk)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "X_test_prediction = nb.predict(X_test)\n",
    "test_data_accuracynb = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"nb->\",test_data_accuracynb)\n",
    "\n",
    "randfor = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "randfor.fit(X_train, y_train)\n",
    "X_test_prediction = randfor.predict(X_test)\n",
    "test_data_accuracyr = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"rf->\",test_data_accuracyr)\n",
    "\n",
    "sampling2 = [test_data_accuracy*100,test_data_accuracyd*100,\n",
    "                                         test_data_accuracyk*100,test_data_accuracynb*100,\n",
    "                                         test_data_accuracyr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b6ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6fd0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9bf02487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr-> 0.9421487603305785\n",
      "dt-> 0.9421487603305785\n",
      "knn-> 0.8305785123966942\n",
      "nb-> 0.8181818181818182\n",
      "rf-> 0.987603305785124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhpuri/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "X_test_prediction = lr.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"lr->\",test_data_accuracy)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "X_test_prediction = dt.predict(X_test)\n",
    "test_data_accuracyd = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"dt->\",test_data_accuracy)\n",
    "\n",
    "knn =  KNeighborsClassifier( n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "X_test_prediction = knn.predict(X_test)\n",
    "test_data_accuracyk = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"knn->\",test_data_accuracyk)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "X_test_prediction = nb.predict(X_test)\n",
    "test_data_accuracynb = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"nb->\",test_data_accuracynb)\n",
    "\n",
    "randfor = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "randfor.fit(X_train, y_train)\n",
    "X_test_prediction = randfor.predict(X_test)\n",
    "test_data_accuracyr = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"rf->\",test_data_accuracyr)\n",
    "\n",
    "sampling3 = [test_data_accuracy*100,test_data_accuracyd*100,\n",
    "                                         test_data_accuracyk*100,test_data_accuracynb*100,\n",
    "                                         0.9122*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af156ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9ef4d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhpuri/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 10\n",
    "random_state = 42\n",
    "\n",
    "# Initialize the KMeans model\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "\n",
    "# Fit the model and get the cluster labels\n",
    "clusters = kmeans.fit_predict(X_resampled)\n",
    "\n",
    "# Initialize an empty list to store the selected sample\n",
    "sample_indices = []\n",
    "\n",
    "# Loop over the clusters and randomly select samples until the sample size is met\n",
    "while len(sample_indices) < 316:\n",
    "    chosen_cluster = random.choice(range(n_clusters))\n",
    "    chosen_indices = list(Y_resampled[clusters == chosen_cluster].index)\n",
    "    sample_indices.extend(random.sample(chosen_indices, min(len(chosen_indices), 316-len(sample_indices))))\n",
    "\n",
    "# Get the cluster sample\n",
    "X_cluster_sample = X_resampled.loc[sample_indices]\n",
    "Y_cluster_sample = Y_resampled.loc[sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1f02af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cluster_sample, Y_cluster_sample, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f921402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr-> 0.9375\n",
      "dt-> 0.9375\n",
      "knn-> 0.8125\n",
      "nb-> 0.828125\n",
      "rf-> 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhpuri/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "X_test_prediction = lr.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"lr->\",test_data_accuracy)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "X_test_prediction = dt.predict(X_test)\n",
    "test_data_accuracyd = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"dt->\",test_data_accuracy)\n",
    "\n",
    "knn =  KNeighborsClassifier( n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "X_test_prediction = knn.predict(X_test)\n",
    "test_data_accuracyk = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"knn->\",test_data_accuracyk)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "X_test_prediction = nb.predict(X_test)\n",
    "test_data_accuracynb = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"nb->\",test_data_accuracynb)\n",
    "\n",
    "randfor = RandomForestClassifier(n_estimators=1, random_state=0)\n",
    "randfor.fit(X_train, y_train)\n",
    "X_test_prediction = randfor.predict(X_test)\n",
    "test_data_accuracyr = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"rf->\",test_data_accuracyr)\n",
    "\n",
    "sampling4 = [test_data_accuracy*100,test_data_accuracyd*100,\n",
    "                                         test_data_accuracyk*100,test_data_accuracynb*100,\n",
    "                                         test_data_accuracyr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72690067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c49cda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "n_splits = 1\n",
    "random_state = 42\n",
    "\n",
    "strat_split = StratifiedShuffleSplit(n_splits=n_splits, test_size=316, random_state=random_state)\n",
    "\n",
    "train_index, test_index = next(strat_split.split(X_resampled, Y_resampled))\n",
    "\n",
    "X_strat_sample = X_resampled.loc[train_index]\n",
    "Y_strat_sample = Y_resampled.loc[train_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0bdfbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_strat_sample,Y_strat_sample, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b6d07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr-> 0.9421487603305785\n",
      "dt-> 0.9421487603305785\n",
      "knn-> 0.8305785123966942\n",
      "nb-> 0.8181818181818182\n",
      "rf-> 0.9173553719008265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhpuri/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "X_test_prediction = lr.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"lr->\",test_data_accuracy)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "X_test_prediction = dt.predict(X_test)\n",
    "test_data_accuracyd = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"dt->\",test_data_accuracy)\n",
    "\n",
    "knn =  KNeighborsClassifier( n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "X_test_prediction = knn.predict(X_test)\n",
    "test_data_accuracyk = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"knn->\",test_data_accuracyk)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "X_test_prediction = nb.predict(X_test)\n",
    "test_data_accuracynb = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"nb->\",test_data_accuracynb)\n",
    "\n",
    "randfor = RandomForestClassifier(n_estimators=1, random_state=0)\n",
    "randfor.fit(X_train, y_train)\n",
    "X_test_prediction = randfor.predict(X_test)\n",
    "test_data_accuracyr = accuracy_score(X_test_prediction, y_test)\n",
    "print(\"rf->\",test_data_accuracyr)\n",
    "\n",
    "sampling5 = [test_data_accuracy*100,test_data_accuracyd*100,\n",
    "                                         test_data_accuracyk*100,test_data_accuracynb*100,\n",
    "                                         test_data_accuracyr*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90bc7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Logistic Regression\",\"Decision Tree\",\"KNN\",\"Gaussian Naive Bayes\",\"Random Forest\"]\n",
    "final_table = pd.DataFrame()\n",
    "final_table['Models'] = models\n",
    "final_table['Systematic'] = sampling1\n",
    "final_table['Simple Random'] = sampling2\n",
    "final_table['Non Sampling'] = sampling3\n",
    "final_table['Cluster'] = sampling4\n",
    "final_table['Stratified'] = sampling5\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c292db48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Systematic</th>\n",
       "      <th>Simple Random</th>\n",
       "      <th>Non Sampling</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Stratified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>57.8125</td>\n",
       "      <td>94.214876</td>\n",
       "      <td>93.7500</td>\n",
       "      <td>94.214876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>67.1875</td>\n",
       "      <td>94.628099</td>\n",
       "      <td>96.8750</td>\n",
       "      <td>94.628099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>43.7500</td>\n",
       "      <td>83.057851</td>\n",
       "      <td>81.2500</td>\n",
       "      <td>83.057851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>43.7500</td>\n",
       "      <td>81.818182</td>\n",
       "      <td>82.8125</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>43.7500</td>\n",
       "      <td>91.220000</td>\n",
       "      <td>93.7500</td>\n",
       "      <td>91.735537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Models  Systematic  Simple Random  Non Sampling  Cluster  \\\n",
       "0   Logistic Regression   83.333333        57.8125     94.214876  93.7500   \n",
       "1         Decision Tree   83.333333        67.1875     94.628099  96.8750   \n",
       "2                   KNN   72.222222        43.7500     83.057851  81.2500   \n",
       "3  Gaussian Naive Bayes   77.777778        43.7500     81.818182  82.8125   \n",
       "4         Random Forest   94.444444        43.7500     91.220000  93.7500   \n",
       "\n",
       "   Stratified  \n",
       "0   94.214876  \n",
       "1   94.628099  \n",
       "2   83.057851  \n",
       "3   81.818182  \n",
       "4   91.735537  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da452ef6",
   "metadata": {},
   "source": [
    "Hence from above table we can find clustered sampling with decision tree for best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30775e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
